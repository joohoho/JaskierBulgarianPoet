{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "16cvVoaSnaUXNw0oxWVxkKwiv4mBZ9wsZ",
      "authorship_tag": "ABX9TyObtIvm3ebrQUou0hziN0cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joohoho/JaskierBulgarianPoet/blob/main/Jaskier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ga3NPsDnwI4r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "import os\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint"
      ],
      "metadata": {
        "id": "nXZR1i-5xyKP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jaskier The Bulgarian Poet"
      ],
      "metadata": {
        "id": "4cI59evAVEZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Course Project for creation of a conversational chatbot in Bulgarian language through preserving text semantics with Word2Vec and Recurrent Neural Network (RNN)"
      ],
      "metadata": {
        "id": "QLaVWMCCVKRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to create a chatbot in the Bulgarian language that converses in rhymes. For this reason, I need to train (and test) a neural network with poems in the Bulgarian language."
      ],
      "metadata": {
        "id": "LxQVxEdhWzpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bulgarian Poems dataset are taken from [kaggle dataset](https://www.kaggle.com/datasets/auhide/bulgarian-poems). Let's have a look at it."
      ],
      "metadata": {
        "id": "4GPuQavg7KN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poems_table = pd.read_csv(\"drive/MyDrive/data/chitanka-corpus.csv\")"
      ],
      "metadata": {
        "id": "9psMsYvk0625"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poems_table.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1C1f5fHo_wJQ",
        "outputId": "9b699054-f2f2-47c5-ce79-5930a891868c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               author                                   title  \\\n",
              "0      Андреас Неуман               (Гравитацията и грацията)   \n",
              "1      Андреас Неуман                (Клаудия в библиотеката)   \n",
              "2      Андреас Неуман                           (Лолита джаз)   \n",
              "3  Александър Радонов                   (Моята) умираща мечта   \n",
              "4        Хорхе Урутия  (Не поверителни) данни за една справка   \n",
              "\n",
              "                                                poem  \n",
              "0  Два съда с вода.<eol>Леко пързаляне с кънки.<e...  \n",
              "1  Издирваш в книгите<eol>със странно усърдие на ...  \n",
              "2  На Хусто Наваро и Хуан де Локса<eol>І<eol>Преч...  \n",
              "3  Като рак ядяща,<eol>самотата подяжда<eol>нечия...  \n",
              "4  че не пиша ръкописвам начертавам зачертавам по...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a7d6c9b-340e-454c-ba9d-2a1cf588605c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>title</th>\n",
              "      <th>poem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Андреас Неуман</td>\n",
              "      <td>(Гравитацията и грацията)</td>\n",
              "      <td>Два съда с вода.&lt;eol&gt;Леко пързаляне с кънки.&lt;e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Андреас Неуман</td>\n",
              "      <td>(Клаудия в библиотеката)</td>\n",
              "      <td>Издирваш в книгите&lt;eol&gt;със странно усърдие на ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Андреас Неуман</td>\n",
              "      <td>(Лолита джаз)</td>\n",
              "      <td>На Хусто Наваро и Хуан де Локса&lt;eol&gt;І&lt;eol&gt;Преч...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Александър Радонов</td>\n",
              "      <td>(Моята) умираща мечта</td>\n",
              "      <td>Като рак ядяща,&lt;eol&gt;самотата подяжда&lt;eol&gt;нечия...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Хорхе Урутия</td>\n",
              "      <td>(Не поверителни) данни за една справка</td>\n",
              "      <td>че не пиша ръкописвам начертавам зачертавам по...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a7d6c9b-340e-454c-ba9d-2a1cf588605c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0a7d6c9b-340e-454c-ba9d-2a1cf588605c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0a7d6c9b-340e-454c-ba9d-2a1cf588605c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4dac628a-35d4-4d14-88ae-3f85e4b6b098\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4dac628a-35d4-4d14-88ae-3f85e4b6b098')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4dac628a-35d4-4d14-88ae-3f85e4b6b098 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "pHUWTOA3_6uQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decide to design a word-level model, so each word in the dataset is a token. We need to preprocess all texts (poems) and clean them from punctuation signs, non-textual symbols (e.g. numbers), and stop words (e.g. words that repeat a lot and bring a small amount of meaning. Let's starts with the preprocessing of the texts."
      ],
      "metadata": {
        "id": "fQKgHWySVB6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make all words in the poems lower\n",
        "\n",
        "poems_table[\"poem\"] = poems_table.poem.str.lower()"
      ],
      "metadata": {
        "id": "lwrJh4_d6PHd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poems_list = poems_table.poem.values.tolist()"
      ],
      "metadata": {
        "id": "DWJT_lTmAAh7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poems_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "dBHkZXkh7gae",
        "outputId": "07c35a28-d23f-4332-bcf0-1dd785a86d8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'два съда с вода.<eol>леко пързаляне с кънки.<eol> <eol>разсъблечени, закръглени,<eol>започваш и завършваш<eol>на краката си:<eol> <eol>съумял си да ги обучиш<eol>в малкото изкуство<eol>да изследват за тебе твърдата земя.<eol> <eol>ще оближа търпеливо<eol>тези точни пръсти.<eol> <eol>(ако беше смислен апетитът ми,<eol>бих закусил с тях,<eol>бели карамели.)<eol> <eol>оставям си тук ботушите.<eol>битките спечелват<eol>страхливите войници.<eol> <eol>гледайки ги, голи,<eol>се уча да ходя.<eol> <eol>събуй ми тези часове,<eol> направи ми път.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a collection of stop words in Bulgarian to apply the cleaning process. The stop words for Bulgarian language are taken from [Stopwords ISO](https://github.com/stopwords-iso/stopwords-bg)."
      ],
      "metadata": {
        "id": "dR2gcaGeg52k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"drive/MyDrive/data/stopwords-bg.txt\", encoding=\"utf8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "lGdg8PQTLVS4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = re.split(\"\\W+\", text)"
      ],
      "metadata": {
        "id": "bkAdtsTlNxNO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set some parameters that we will need further on."
      ],
      "metadata": {
        "id": "z-42L9MY7yx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_texts = max(poems_table.poem.index)\n",
        "min_occurance = 20\n",
        "vector_size = 20\n",
        "seed = 11"
      ],
      "metadata": {
        "id": "KzWSayyECb6X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a function to gather all the preproccessed words together. Then we will make a set of unique words out of it."
      ],
      "metadata": {
        "id": "TDE5Sk5y83A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a corpus with repeating preprocessed words\n",
        "\n",
        "all_poems = []\n",
        "\n",
        "def make_corpus(data):\n",
        "  \"\"\"\n",
        "  This is a function to gather all the preprocessed words together to make a dictionary.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  Data to be processed. Should be a list.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  A list object that is a dictionary with all the words of interest from the texts\n",
        "  \"\"\"\n",
        "  for i in range(num_texts):\n",
        "\n",
        "    text = data[i].split()\n",
        "    text = [word for word in text if word not in string.punctuation] # Removing punctuations\n",
        "    text = [word for word in text if word.isalpha()] # Removing all the words having characters other than letters\n",
        "    text = [word for word in text if word not in stopwords] #Removing all the stop words\n",
        "\n",
        "    seq = ' '.join(text)\n",
        "    split_seq = seq.split()\n",
        "\n",
        "    for index in range(len(split_seq)):  #Pputting all the words together\n",
        "      all_poems.append(split_seq[index])\n",
        "\n",
        "  return all_poems"
      ],
      "metadata": {
        "id": "wpYco58oCtiK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are all the prepreccessed words with repeating\n",
        "\n",
        "corpus = make_corpus(poems_list)\n",
        "corpus[0:100]"
      ],
      "metadata": {
        "id": "yzddeAuItdeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3196b7c5-e74b-4f67-e71d-75057120fd19"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['съда',\n",
              " 'пързаляне',\n",
              " 'краката',\n",
              " 'малкото',\n",
              " 'изследват',\n",
              " 'тебе',\n",
              " 'твърдата',\n",
              " 'оближа',\n",
              " 'точни',\n",
              " 'смислен',\n",
              " 'апетитът',\n",
              " 'закусил',\n",
              " 'уча',\n",
              " 'издирваш',\n",
              " 'странно',\n",
              " 'усърдие',\n",
              " 'профил',\n",
              " 'начин',\n",
              " 'свиваш',\n",
              " 'напрягаш',\n",
              " 'бавна',\n",
              " 'татуировка',\n",
              " 'кръста',\n",
              " 'отколкото',\n",
              " 'обрисуваш',\n",
              " 'осмелява',\n",
              " 'свързват',\n",
              " 'думата',\n",
              " 'нейния',\n",
              " 'преструвам',\n",
              " 'обърнеш',\n",
              " 'поглед',\n",
              " 'твърде',\n",
              " 'правиш',\n",
              " 'хусто',\n",
              " 'наваро',\n",
              " 'хуан',\n",
              " 'де',\n",
              " 'капризно',\n",
              " 'изобилие',\n",
              " 'коляното',\n",
              " 'точното',\n",
              " 'отражение',\n",
              " 'ръкавица',\n",
              " 'непромокаемия',\n",
              " 'шлифер',\n",
              " 'уста',\n",
              " 'изисква',\n",
              " 'своя',\n",
              " 'тъмната',\n",
              " 'страна',\n",
              " 'неувереност',\n",
              " 'леката',\n",
              " 'страст',\n",
              " 'започнеш',\n",
              " 'обичаш',\n",
              " 'my',\n",
              " 'мента',\n",
              " 'моят',\n",
              " 'орех',\n",
              " 'езика',\n",
              " 'става',\n",
              " 'бръмбарче',\n",
              " 'моя',\n",
              " 'връхчето',\n",
              " 'my',\n",
              " 'запалиш',\n",
              " 'невидимо',\n",
              " 'обсипано',\n",
              " 'лъже',\n",
              " 'пясъчният',\n",
              " 'часовник',\n",
              " 'отидеш',\n",
              " 'мене',\n",
              " 'твоя',\n",
              " 'неначенати',\n",
              " 'тестета',\n",
              " 'порокът',\n",
              " 'ягодови',\n",
              " 'асото',\n",
              " 'спечели',\n",
              " 'приличаше',\n",
              " 'ухание',\n",
              " 'my',\n",
              " 'charming',\n",
              " 'little',\n",
              " 'дъжд',\n",
              " 'ню',\n",
              " 'похарчих',\n",
              " 'чака',\n",
              " 'приятел',\n",
              " 'бърлогата',\n",
              " 'лолита',\n",
              " 'разсеяно',\n",
              " 'рак',\n",
              " 'мечта',\n",
              " 'умря',\n",
              " 'умира',\n",
              " 'самотник',\n",
              " 'умирам']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(corpus), len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAfFRekXiXJH",
        "outputId": "f4bf6ac7-ab5b-4fb0-e191-69b439cf326b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, 572629)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a unique set of words - our dictionary\n",
        "\n",
        "set_corpus = set(corpus)"
      ],
      "metadata": {
        "id": "QxsjPQOLC3s1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tktSd40cqQP4",
        "outputId": "84f2c97b-ade2-4a45-a4fc-9ff180aae233"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81153"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a vocabulary that shows an index for each unique word. And also an inverse vocabulary."
      ],
      "metadata": {
        "id": "oMh4QLjAASnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary\n",
        "\n",
        "vocab, index = {}, 1  # start indexing from 1\n",
        "vocab[\"<UNK>\"] = 0  # add a unknown token\n",
        "for token in set_corpus:\n",
        "  if token not in vocab:\n",
        "    vocab[token] = index\n",
        "    index += 1"
      ],
      "metadata": {
        "id": "61VytFN7kTt6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fski4pm1qUiT",
        "outputId": "76f0ddc6-eea3-4705-e5e6-080ee2742c10"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81154"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[\"дръзнах\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-O3573zUlWO",
        "outputId": "fe204aae-2da4-4025-d0d2-7364f00d53ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57964"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an inverse vocabulary\n",
        "\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}"
      ],
      "metadata": {
        "id": "pEEAcqFKkrrS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab[vocab[\"дръзнах\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rKdyHUESUwzH",
        "outputId": "e1774a39-9786-42b7-bd6d-eaaf612b6513"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'дръзнах'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would be preprocessing our poems again keeping only the words that are present in our dictionary list."
      ],
      "metadata": {
        "id": "F4tZfpUSZiaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_corpus_sequences(data):\n",
        "  \"\"\"\n",
        "  This is a function to clean the texts and leave words of interest in sequences.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  data - the text to be cleaned up.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  A cleaned sequence with elements that match preproccessing rules.\n",
        "  \"\"\"\n",
        "  text = data.split()\n",
        "  text = [word for word in text if word in set_corpus] # Removing words that are not in \"my_dict\"\n",
        "\n",
        "  seq = ' '.join(text)\n",
        "  seq = seq.split()\n",
        "\n",
        "  return seq"
      ],
      "metadata": {
        "id": "IliiHJjXVLj-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_sequences = []\n",
        "for i in range(num_texts):\n",
        "    clean_text = make_corpus_sequences(poems_table.poem[i]) # cleaning up the poem\n",
        "    corpus_sequences.append(clean_text) # adding the preprocessed poem to our list"
      ],
      "metadata": {
        "id": "gvl7JVvbZeZo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to encode each word as a sequence of integers. We need to present the tokens (words) as a vector with numbers. There are many ways to do it. We can simply give a unique number of each word and then make it as a one-hot-encoded vector with fixed length, using padding. However this won't preserve the place of that word in our latent space. Those vectors won't have any idea about the semantics of our texts.\n",
        "\n",
        "That's why I decided to use word2vec algorithm."
      ],
      "metadata": {
        "id": "A4Bqz574Ddcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec(corpus_sequences, min_count = min_occurance, vector_size = vector_size, workers = 3, seed = seed)"
      ],
      "metadata": {
        "id": "zjs_JhkyR5F5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JxUzn7khA38",
        "outputId": "e3ff1303-e04a-4cdf-cf11-a99eec2e7207"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=4823, vector_size=20, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2vec.wv)"
      ],
      "metadata": {
        "id": "D0mDpQiFVGDo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I reduce the size of the vocabulary, by using the hyperparameter \"min_count\", set to 20. This excludes all the words that are with frequency in the corpus, less than that. From initial 81153 to 4823 words to be trained."
      ],
      "metadata": {
        "id": "y8PQdewZSTLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.train(corpus_sequences, total_examples = num_texts, epochs = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C2p0aJaSoKO",
        "outputId": "6fa691d5-fa1c-4d68-953f-d7c3a7874ce5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3565477, 5726290)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the model in \"bin\" format."
      ],
      "metadata": {
        "id": "fyLeToBJShlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.save(\"/content/drive/MyDrive/preprocessed_data_Jaskier/word2vec.bin\")"
      ],
      "metadata": {
        "id": "K6633LmFSfb4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model and check similar and dissimilar words."
      ],
      "metadata": {
        "id": "JnjffaBPTPbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec.load(\"/content/drive/MyDrive/preprocessed_data_Jaskier/word2vec.bin\")"
      ],
      "metadata": {
        "id": "0Gw0a_xNSoM4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = word2vec.wv.most_similar(\"лазур\")\n",
        "similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoseyPAFTSUE",
        "outputId": "42df6f06-cc52-45cf-fbe7-8e87933142de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('сребърна', 0.9492636919021606),\n",
              " ('вечерен', 0.9452553987503052),\n",
              " ('есенен', 0.9373318552970886),\n",
              " ('лунен', 0.9320693016052246),\n",
              " ('зрак', 0.929793655872345),\n",
              " ('вечерна', 0.9293441772460938),\n",
              " ('дъхът', 0.9286230206489563),\n",
              " ('лунните', 0.9264757037162781),\n",
              " ('унес', 0.9253934621810913),\n",
              " ('просторите', 0.9234907031059265)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dissimlar_words = word2vec.wv.doesnt_match(\"чуваш първите стъпки поглед\".split())\n",
        "dissimlar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ksmhw4ZSTSfg",
        "outputId": "7c4c060f-7f02-4326-cc4d-dcef0f16a817"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'поглед'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_sample = word2vec.wv[\"мирис\"]\n",
        "vector_sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcV9B2ukTciE",
        "outputId": "0f33fbf2-5b62-4581-ace1-08ebf3b9d854"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.84118867, -1.0650666 , -0.40173572, -0.57912475, -0.6908556 ,\n",
              "       -0.00289592,  0.9373853 ,  0.1081099 ,  1.1116147 , -0.9885714 ,\n",
              "       -0.08186585, -0.549227  ,  0.7389428 , -0.16801046,  0.85311544,\n",
              "       -0.5582539 ,  0.08983558, -0.1169704 ,  0.12766753,  0.16724469],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's save and reload the vectors\n",
        "\n",
        "word2vec.wv.save(\"/content/drive/MyDrive/preprocessed_data_Jaskier/vectors.kv\")\n",
        "word_vectors = KeyedVectors.load(\"/content/drive/MyDrive/preprocessed_data_Jaskier/vectors.kv\")"
      ],
      "metadata": {
        "id": "GcVJavHgm7gW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oysdtcoYoDdV",
        "outputId": "079a5c84-8c92-4cd7-acdd-fceedb3befeb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.2204913 , -2.6143334 ,  0.72949004, -0.42397836, -0.11209559,\n",
              "        0.14924559, -0.1384469 , -0.01623715,  0.2194805 , -0.43451595,\n",
              "       -0.8399127 , -0.14398257,  1.4419646 , -1.148511  , -1.4091693 ,\n",
              "       -1.8123314 ,  2.943314  ,  2.8133228 ,  0.3327512 , -1.445299  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained weights are in model.wv.vectors, which is a 2D matrix of shape (number of words, dimensionality of word vectors)."
      ],
      "metadata": {
        "id": "-Q2uLrBFw7Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weigths = word2vec.wv.vectors\n",
        "weigths.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DL4BlC8w6U4",
        "outputId": "d6b2223e-6825-4bd7-a523-a2be6789b1ec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4823, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mapping between the word indices in this matrix (integers) and the words themselves (strings) is in model.wv.index_to_key."
      ],
      "metadata": {
        "id": "kpouQdYnxmrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# brings the original list of words / our vocabulary\n",
        "\n",
        "word2vec.wv.index_to_key[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dpVMQvqck4DH",
        "outputId": "c4cad038-667d-4cd5-89be-f6076e4d4d79"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'очите'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test Split"
      ],
      "metadata": {
        "id": "W7eO8mBB3LMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test propertion is approximately 15%\n",
        "\n",
        "poems_test = corpus[0:85880]\n",
        "len(poems_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi5Gol9llcL6",
        "outputId": "88d0c491-a9e6-415d-ee8f-43399905f08b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85880"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poems_train = corpus[85880:572620]\n",
        "len(poems_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8X2TVmDtDVg",
        "outputId": "c112c44f-4a9e-420d-b2f6-870fbd1d136e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "486740"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = corpus[0:50]"
      ],
      "metadata": {
        "id": "vjamwr2uLvpC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del corpus"
      ],
      "metadata": {
        "id": "dQxLEJvYu7Xr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize data"
      ],
      "metadata": {
        "id": "ljx2EKYU04PU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each poem has a different length, due to the text origin itself. Unfortunately, machine learning doesn't understand speech logic and we need to find a way to make each piece of text equal in length, e.g. with the same shape of the vector that represents it.\n",
        "\n",
        "This simple task appeared to be much more complicated at a second sight."
      ],
      "metadata": {
        "id": "0TufbN7sv7Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My first approach was to take word2vec vectors in the sequences of the text and average them to get one vector of a fixed second dimension per text. However I have restraints with this approach as I don't see the logic to feed the model with something that is close to all words, but is not exactly any of them. So I decided to save the weights from the trained word2vec model in an Embedding layer and to feed my model with dense vectors."
      ],
      "metadata": {
        "id": "6VaBCSGKFySI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I continued with functions creation for preproccessing."
      ],
      "metadata": {
        "id": "xqHhQYtJFFD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuction to iterate over the poems' words and return dense vectors to feed the model\n",
        "\n",
        "vectors_list = []\n",
        "\n",
        "def make_dense_vectors(data):\n",
        "\n",
        "  for i in range(0, len(data)):\n",
        "    if data[i] not in vocab.keys():\n",
        "      number = 0\n",
        "    else:\n",
        "      number = vocab[data[i]]\n",
        "\n",
        "    vectors_list.append(number)\n",
        "\n",
        "  return vectors_list"
      ],
      "metadata": {
        "id": "x-gYPQTdSUgV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make input chunks of data\n",
        "\n",
        "input_chunks_list = []\n",
        "\n",
        "def make_input_chunks(data):\n",
        "  num_elements = len(make_dense_vectors(data))\n",
        "  step = chunk_size\n",
        "\n",
        "  for i in range(0, num_elements, step):\n",
        "    chunks = make_dense_vectors(data)[i: i + chunk_size]\n",
        "    input_chunks_list.append(chunks)\n",
        "\n",
        "  if len(input_chunks_list[-1]) < 4:\n",
        "    del input_chunks_list[-1]\n",
        "\n",
        "  return input_chunks_list"
      ],
      "metadata": {
        "id": "Xt_2J6ryU54B"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make target chunks of data\n",
        "\n",
        "target_chunks_list = []\n",
        "\n",
        "def make_target_chunks(data):\n",
        "  num_elements = len(make_dense_vectors(data))\n",
        "  step = chunk_size\n",
        "\n",
        "  for i in range(4, num_elements, step):\n",
        "    chunks = make_dense_vectors(data)[i: i + chunk_size]\n",
        "    target_chunks_list.append(chunks)\n",
        "\n",
        "  if len(target_chunks_list[-1]) < 4:\n",
        "    del target_chunks_list[-1]\n",
        "\n",
        "  target_chunks_list.append([0,0,0,0])\n",
        "\n",
        "  return target_chunks_list"
      ],
      "metadata": {
        "id": "rJGBOVcHliup"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 4"
      ],
      "metadata": {
        "id": "InqIeOyk-26b"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = make_dense_vectors(test)"
      ],
      "metadata": {
        "id": "y2JC3duM3XqC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = (list(a))"
      ],
      "metadata": {
        "id": "Mz7ealVlBXrE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.extend([0,0,0,0])"
      ],
      "metadata": {
        "id": "jRYHBx8-lrJG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(a, dtype = \"float32\")"
      ],
      "metadata": {
        "id": "ZUWgX0vy6osH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(b[4:], dtype = \"float32\")"
      ],
      "metadata": {
        "id": "6KamcjyX-gwh"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPpZKxvP65f1",
        "outputId": "1444ef25-eef7-419d-915d-6219305c1d0c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50,), (50,))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to one-hot encode the target_chunks list to be ready to be fitted in the model."
      ],
      "metadata": {
        "id": "Wi8_UJ1Tn8bd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model creation using word2vec embeddings\n",
        "\n",
        "We need to create a sequence to sequence model for next token prediction. The architecture I chose is model that uses encoder-decoder structure, introduced by Google. The encoder processes the input sequence and transforms it into a fixed-size hidden representation. The decoder uses the hidden representation to generate output sequence. The encoder-decoder structure allows them to handle input and output sequences of different lengths, making them capable to handle sequential data."
      ],
      "metadata": {
        "id": "Htmgg_P2OUWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. A RNN layer acts as \"encoder\": it processes the input sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n",
        "2. Another RNN layer acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence."
      ],
      "metadata": {
        "id": "dbkr4Do8dIK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
        "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_embeddings : bool\n",
        "        If False, the returned weights are frozen and stopped from being updated.\n",
        "        If True, the weights can / will be further updated in Keras.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    \"keras.layers.Embedding\"\n",
        "        Embedding layer, to be used as input to deeper network layers.\n",
        "\n",
        "    \"\"\"\n",
        "    keyed_vectors = model.wv  # structure holding the result of training\n",
        "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array\n",
        "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
        "\n",
        "    layer = Embedding(\n",
        "        input_dim = weights.shape[0],\n",
        "        output_dim = weights.shape[1],\n",
        "        weights = [weights],\n",
        "        trainable = train_embeddings,\n",
        "    )\n",
        "    return layer"
      ],
      "metadata": {
        "id": "7KLavd8At913"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weigths.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO6tzzelu4iP",
        "outputId": "0af041db-dcbb-4fa1-88d1-ecee6d28df72"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4823, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1  # Batch size for training\n",
        "epochs = 10  # Number of epochs to train for\n",
        "latent_dim = 128  # Latent dimensionality of the encoding space\n",
        "num_decoder_tokens = vocab_size"
      ],
      "metadata": {
        "id": "i6EuEy9jgFH9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data, the model works with\n",
        "\n",
        "# encoder_input_data = np.zeros((1000, 7), dtype=\"float32\")\n",
        "# decoder_input_data = np.zeros((1000, 16), dtype=\"float32\")\n",
        "# decoder_target_data = np.zeros((1000, 16, num_decoder_tokens), dtype=\"float32\")"
      ],
      "metadata": {
        "id": "FRum5wQgbGNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ],
      "metadata": {
        "id": "SKRiRcDd7A8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most efficient way is to create a pipeline for proccessing the data. It ensures the reusability of the workflow and also gives us a fast execution."
      ],
      "metadata": {
        "id": "STIFZQPLFlaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = tf.data.Dataset.from_tensor_slices(x) \\\n",
        "  .batch(4, drop_remainder=True)"
      ],
      "metadata": {
        "id": "l6VBjorDva6K"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input_data = tf.data.Dataset.from_tensor_slices(y) \\\n",
        "  .batch(4, drop_remainder = True)"
      ],
      "metadata": {
        "id": "rkbbKsg9oat9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(vector):\n",
        "  vector = tf.one_hot(y, depth = num_decoder_tokens)\n",
        "  return vector"
      ],
      "metadata": {
        "id": "DVwJ8ROWp4Kd"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data = tf.data.Dataset.from_tensor_slices(y) \\\n",
        "  .map(one_hot_encode) \\\n",
        "  .batch(4, drop_remainder = True)"
      ],
      "metadata": {
        "id": "2zwvXiN-odmk"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data.element_spec, decoder_input_data.element_spec, decoder_target_data.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj0weGlYiCIK",
        "outputId": "c6a71d69-3b22-4fbd-a1fc-bdc95cd19f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(4,), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(4,), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(4, 50, 4823), dtype=tf.float32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately with this approach I keep getting the following error:\n",
        "\n",
        "ValueError: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\"}), <class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\n",
        "\n",
        "I spent huge amount of time debugging it without a success."
      ],
      "metadata": {
        "id": "ANlASt_h_ZqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also tryied to make a generator and save smaller batches of the preproccessed data.\n",
        "\n",
        "However the last approach didn't have any success as the functions I used are extremely slow and not good to have in your project. They consume high RAM and takes forever to end."
      ],
      "metadata": {
        "id": "n4vtHmoADjgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example of this \"poor\" code."
      ],
      "metadata": {
        "id": "APhEjyPDD_fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "x = gensim_to_keras_embedding(word2vec)(encoder_inputs)\n",
        "x, state_h, state_c = LSTM(latent_dim,\n",
        "                           return_state=True)(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "x = Embedding(num_decoder_tokens, latent_dim)(decoder_inputs)\n",
        "x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n",
        "\n",
        "# Define the model that will turn \"encoder_input_data\" and \"decoder_input_data\" into \"decoder_target_data\"\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "_akberA2eqy-"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and run training\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001), loss=\"categorical_crossentropy\")"
      ],
      "metadata": {
        "id": "NFmLuHqodEz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtAXTKRGMu4L",
        "outputId": "c3d5594d-92ea-4fdf-81a0-e9cab87349fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 20)             96460     ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 128)            617344    ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 128),                76288     ['embedding[0][0]']           \n",
            "                              (None, 128),                                                        \n",
            "                              (None, 128)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, None, 128)            131584    ['embedding_1[0][0]',         \n",
            "                                                                     'lstm[0][1]',                \n",
            "                                                                     'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 4823)           622167    ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1543843 (5.89 MB)\n",
            "Trainable params: 1447383 (5.52 MB)\n",
            "Non-trainable params: 96460 (376.80 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "3ykbBkpGNDvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that \"decoder_target_data\" needs to be one-hot encoded, rather than sequences of integers like \"decoder_input_data\"!\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          # batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[ModelCheckpoint(\"./checkpoints/\"), tensorboard_callback],\n",
        "          # validation_split=0.2,\n",
        "          shuffle = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Ydi4a99AVqTd",
        "outputId": "a1c5cc66-99ae-4a00-a8fd-179114152e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\"}), <class 'tensorflow.python.data.ops.batch_op._BatchDataset'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-750de611e7a8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note that \"decoder_target_data\" needs to be one-hot encoded, rather than sequences of integers like \"decoder_input_data\"!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0;31m# batch_size=batch_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./checkpoints/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1106\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m   1107\u001b[0m                 \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\"}), <class 'tensorflow.python.data.ops.batch_op._BatchDataset'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"./checkpoints/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "da_V_x-YnIQE",
        "outputId": "119d5f98-65c4-4883-8dd8-1001323d040e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: './checkpoints/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-f6c73025c5ef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./checkpoints/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './checkpoints/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save()"
      ],
      "metadata": {
        "id": "3KAljJcME0JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "6NaohNmj1--1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict()"
      ],
      "metadata": {
        "id": "5dbAVqs5RiM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We receive for every vector another vector with probabilities."
      ],
      "metadata": {
        "id": "2JllnBO7cpXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.argmax(predictions, axis = 1)"
      ],
      "metadata": {
        "id": "Ui00GHRXQev2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorBoard now shows the word2vec model's accuracy and loss:"
      ],
      "metadata": {
        "id": "Zi_A0LDAopd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# docs_infra: no_execute\n",
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "2kcMxK1Fqpa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "59UNpDinkN5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Articles of Tomas Mikolov:\n",
        "\n",
        "[1. First](https://arxiv.org/abs/1301.3781)\n",
        "\n",
        "[2. Second](https://arxiv.org/abs/1310.4546)"
      ],
      "metadata": {
        "id": "WpToBzrNkWXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Word2Vec Documentation](https://radimrehurek.com/gensim/apiref.html)"
      ],
      "metadata": {
        "id": "sijI9IakqNFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Piskvorky Git Repo](https://github.com/piskvorky/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow)"
      ],
      "metadata": {
        "id": "kmMnaAiHzJhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Article \"Sequence to Sequence Learning with Neural Networks\"](https://arxiv.org/abs/1409.3215)"
      ],
      "metadata": {
        "id": "j7JIvh8sZN6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Machine Learning Mastery](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)"
      ],
      "metadata": {
        "id": "SR6_pauuqNTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Medium Article](https://medium.com/@dilip.voleti/classification-using-word2vec-b1d79d375381)"
      ],
      "metadata": {
        "id": "67waBzyO0vQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Keras Guide to Seq2Seq models](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)"
      ],
      "metadata": {
        "id": "wiACcKpB0vi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Seq2Seq model](https://cnvrg.io/seq2seq-model/)"
      ],
      "metadata": {
        "id": "OG3HUzfM0t97"
      }
    }
  ]
}